{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe25ad10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\Patrick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\Patrick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\Patrick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio transformers accelerate peft --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c362e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\Patrick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\Patrick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ympy (C:\\Users\\Patrick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074303a",
   "metadata": {},
   "source": [
    "# Academic Writing LoRa Model: \n",
    "<br>\n",
    "<p>This Project creates an Academic Writing Assistant that generates text continuations for the user in an academic-style.</p>\n",
    "<p>\n",
    "The idea for this project started in code, CoPilot has been a massive success that has aided millions of people in coding. Providing real time assistance, as well as an auto-complete function for coding based on previous works of the user. This, significantly improving the experience of users, has not been applied to a wide range in natural language. </p>\n",
    "<p>\n",
    "While autocomplete has been around for a while, one area wherein text continuation has not been taken advantage of is in academic writing -- As Copilot has provided a large quality of life experience for those in coding, this project aims to do the same for academic writers. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50b2c2",
   "metadata": {},
   "source": [
    "# Why Lora? \n",
    "<p>\n",
    "Lora is a great fit for the initial task of training and fine tuning the model as the original task of the code(text continuation) can already be done on a surface level. \n",
    "</p>\n",
    "<p>\n",
    "The main adaptation of this code is to create a text continuation for academic writing -- That is, training a model for the specific task of writing for academic writings and scholarly papers. In the context of this code, this means that Lora is perfect for us as it allows us to fine tune an already proven and working base model (AutoModeforCausalM) and create our academic writing auto continuation without the exceedingly high computational costs of training an entire base model by ourselves.  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c28a29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34bbd110fc764fca86d6b9fa88dc756c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Patrick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Patrick\\.cache\\huggingface\\hub\\models--EleutherAI--gpt-neo-125M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186a71ac0be24a7897ab4ae22fa4aa09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d29fb492ca4658b2d3bd2267e071f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Patrick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "\n",
    "# Load LoRA model using the approach from model_evaluation_and_finetuning.ipynb\n",
    "model_name = \"./lora_academic_model\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model = PeftModel.from_pretrained(base_model, model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7b775",
   "metadata": {},
   "source": [
    "### TESTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd04ff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Deep reinforcement learning has been widely adopted in robotic navigation.\n",
      "\n",
      "Generated Continuation: The goal of this paper is to provide a novel algorithm for learning a novel robotic navigation system. The algorithm is composed of a first-order gradient descent (GAD) algorithm with a learning rate, a second-order loss function, and a third-order loss function. The algorithm is implemented on an artificial neural network (ANN) based on a multi-layer perceptron (MLP) and a single-layer perceptron (SLP). The algorithm is evaluated by a simulation study and compared\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Ensure the fine-tuned model and tokenizer are loaded (from cell AlY-JVt2g7M6)\n",
    "# If they are not in scope, you might need to run cell AlY-JVt2g7M6 first.\n",
    "\n",
    "# Define a context for generation\n",
    "# Using the same context as the stylistic control example\n",
    "context = \"Deep reinforcement learning has been widely adopted in robotic navigation.\"\n",
    "\n",
    "# Tokenize the input context\n",
    "inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate continuation using the fine-tuned model\n",
    "# You can adjust max_new_tokens, do_sample, temperature, top_p as needed\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask, # Explicitly pass attention_mask\n",
    "    max_new_tokens=100, # Generate up to 100 new tokens\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,      # Enable sampling for more diverse outputs\n",
    "    temperature=0.7,     # Control creativity (lower for less, higher for more)\n",
    "    top_p=0.9,           # Nucleus sampling\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the generated tokens, skipping the input context and special tokens\n",
    "generated_text = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"Context:\", context)\n",
    "print(\"\\nGenerated Continuation:\", generated_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10424a7d",
   "metadata": {},
   "source": [
    "### FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b32b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LoRA model with sample prompts:\n",
      "\n",
      "Context: Deep reinforcement learning has been widely adopted in robotic navigation.\n",
      "\n",
      "Generated Continuation: However, the time required to train the algorithm is relatively high, especially for deep learning, and thus the performance of the algorithm is degraded. The reason is that the network is designed to learn to perform the task, while the training process is relatively complex and therefore it is difficult to train the algorithm.\n",
      "To address this, a recent method of applying a network to the training process has been proposed.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Context: The key findings of this research are\n",
      "\n",
      "Generated Continuation: : (i) The incidence of chronic obstructive pulmonary disease in Iran is higher than in other developing countries, (ii) The prevalence of chronic obstructive pulmonary disease in Iran is higher than in other developing countries, (iii) The prevalence of chronic obstructive pulmonary disease in Iran is higher than in other developing countries, (iv) The prevalence of chronic obstructive pulmonary disease in Iran is higher than\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Context: In recent years, machine learning has demonstrated\n",
      "\n",
      "Generated Continuation: that it is possible to perform large-scale training on thousands of training examples. In particular, machine learning methods have been developed for the recognition of complex, multi-dimensional features in data. The ability to learn a feature representation (such as a weight) has also been realized.\n",
      "A feature representation can be represented by a training set of training examples. In this case, a representation of a feature\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_continuation(context, max_new_tokens=100, temperature=0.7, top_p=0.9, print_result=True):\n",
    "    \"\"\"\n",
    "    Generate academic text continuation using the loaded LoRA model.\n",
    "    \n",
    "    Args:\n",
    "        context (str): Input prompt/context for generation\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate (default: 100)\n",
    "        temperature (float): Sampling temperature - higher for more creativity (default: 0.7)\n",
    "        top_p (float): Nucleus sampling parameter (default: 0.9)\n",
    "        print_result (bool): Whether to print the input and output (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated continuation text\n",
    "    \"\"\"\n",
    "    # Tokenize the input context\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate continuation using the fine-tuned model\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens, skipping the input context and special tokens\n",
    "    generated_text = tokenizer.decode(\n",
    "        output_ids[0][inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    if print_result:\n",
    "        print(\"Context:\", context)\n",
    "        print(\"\\nGenerated Continuation:\", generated_text.strip())\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "# Test the function with sample prompts\n",
    "test_prompts = [\n",
    "    \"Deep reinforcement learning has been widely adopted in robotic navigation.\",\n",
    "    \"The key findings of this research are\",\n",
    "    \"In recent years, machine learning has demonstrated\"\n",
    "]\n",
    "\n",
    "print(\"Testing LoRA model with sample prompts:\\n\")\n",
    "for prompt in test_prompts:\n",
    "    generate_continuation(prompt, max_new_tokens=80)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb52b0b",
   "metadata": {},
   "source": [
    "# Lora Model Eval\n",
    "<p>\n",
    "Now that we've built our model we can perform evaluation on it to test our hypothesis of this Lora Model being more successful. <br>\n",
    "Same as the last evaluation we are going to test the model against the reference paragraph to see if the generated predictions are valid in the context of the references. <br>\n",
    "The sentences given are written arbitrarily to further test the system. <br>\n",
    "\n",
    "<p> \"The system collects raw input data and converts it into a standardized format for processing. A central module then applies predefined rules to analyze the data and generate intermediate results. Finally, the output component compiles these results into a structured report that can be used by downstream applications.\" <br>\n",
    "\"The application monitors incoming signals and filters out any values that fall outside the expected range. It then applies a series of transformation steps to extract relevant features from the remaining data. These features are stored in a shared buffer, allowing other system components to access them efficiently.\" <br><br>\n",
    "adding to these, we will be using a casual-tone paragraph to check if there has been any deviation from the usual model: <br>\n",
    "\"I checked the system this morning, and everything seemed to be running smoothly. There were a few slow moments, but nothing that looked serious or out of the ordinary. If anything changes, I‚Äôll take another look and make sure it‚Äôs all sorted.\" </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a4b5c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LoRA model with sample prompts:\n",
      "\n",
      "Context: The system collects raw input data and converts it into a standardized format for processing.\n",
      "\n",
      "Generated Continuation: In other words, the system is a computer that can run a variety of applications. These applications are typically applications for which the input data is processed in a variety of ways.\n",
      "\n",
      "The system collects raw input data and converts it into a standardized format for processing.\n",
      "\n",
      "For example, a system may collect raw input data from a user‚Äôs computer (e.g., an\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Context: The application monitors incoming signals and filters out any values that fall outside the expected range.\n",
      "\n",
      "Generated Continuation: For example, if the signal is a random number between 0 and 1, then it is possible to check whether it is positive or negative. If it is positive, then it is possible to determine whether it is positive or negative.\n",
      "\n",
      "As the signal is sent over the Internet, it is possible to send a number of different data, such as a random number, to the processor.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Context: I checked the system this morning, and everything seemed to be running smoothly\n",
      "\n",
      "Generated Continuation: .\n",
      "\n",
      "It‚Äôs a good time to do a little more research on the topic of ‚Äúhow to do something that makes sense‚Äù.\n",
      "\n",
      "I‚Äôm a bit of a writer, so I want to know how to get some of the ‚Äústuff‚Äù I‚Äôve been using in my head. So far I‚Äôve been using the ÔøΩ\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prompts = [\n",
    "    \"The system collects raw input data and converts it into a standardized format for processing.\",\n",
    "    \"The application monitors incoming signals and filters out any values that fall outside the expected range.\",\n",
    "    \"I checked the system this morning, and everything seemed to be running smoothly\"\n",
    "]\n",
    "\n",
    "print(\"Testing LoRA model with sample prompts:\\n\")\n",
    "for prompt in test_prompts:\n",
    "    generate_continuation(prompt, max_new_tokens=80)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "856ec8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Patrick\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load as load_evaluate\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# BLEU requires punkt tokenizer\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def evaluate_predictions(predictions, references):\n",
    "    \"\"\"\n",
    "    Compare model predictions with ground truth using:\n",
    "    - BERTScore\n",
    "    - ROUGE (1,2,L)\n",
    "    - BLEU\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure matching lengths\n",
    "    assert len(predictions) == len(references), \"Prediction and reference length mismatch.\"\n",
    "\n",
    "    # ============================\n",
    "    # 1. BERTScore\n",
    "    # ============================\n",
    "    bertscore = load_evaluate(\"bertscore\")\n",
    "    bert_res = bertscore.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        model_type=\"bert-base-uncased\"\n",
    "    )\n",
    "    bert_precision = np.mean(bert_res[\"precision\"])\n",
    "    bert_recall = np.mean(bert_res[\"recall\"])\n",
    "    bert_f1 = np.mean(bert_res[\"f1\"])\n",
    "\n",
    "    # ============================\n",
    "    # 2. ROUGE\n",
    "    # ============================\n",
    "    rouge = load_evaluate(\"rouge\")\n",
    "    rouge_res = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "    )\n",
    "\n",
    "    # ============================\n",
    "    # 3. BLEU (corpus)\n",
    "    # ============================\n",
    "    from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "    # NLTK corpus BLEU expects tokenized inputs ‚ûô we will use word_tokenize\n",
    "    tokenized_preds = [nltk.word_tokenize(p) for p in predictions]\n",
    "    tokenized_refs = [[nltk.word_tokenize(r)] for r in references]\n",
    "\n",
    "    bleu_score = corpus_bleu(tokenized_refs, tokenized_preds)\n",
    "\n",
    "    # ============================\n",
    "    # Final Output\n",
    "    # ============================\n",
    "    return {\n",
    "        \"bertscore_precision\": bert_precision,\n",
    "        \"bertscore_recall\": bert_recall,\n",
    "        \"bertscore_f1\": bert_f1,\n",
    "        \"rouge1\": rouge_res[\"rouge1\"],\n",
    "        \"rouge2\": rouge_res[\"rouge2\"],\n",
    "        \"rougeL\": rouge_res[\"rougeL\"],\n",
    "        \"bleu\": bleu_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b246dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "BERTScore Precision: 0.6731\n",
      "BERTScore Recall: 0.6117\n",
      "BERTScore F1: 0.6400\n",
      "ROUGE-1: 0.4147\n",
      "ROUGE-2: 0.2578\n",
      "ROUGE-L: 0.3586\n",
      "BLEU: 0.2055\n"
     ]
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"The system collects raw input data and converts it into a standardized format for processing. In other words, the system is a computer that can run a variety of applications. These applications are typically applications for which the input data is processed in a variety of ways.\",\n",
    "    \"The application monitors incoming signals and filters out any values that fall outside the expected range. It then applies a series of transformation steps to extract relevant features from the remaining data. These features are stored in a shared buffer, allowing other system components to access them efficiently.\",\n",
    "    \"I checked the system this morning, and everything seemed to be running smoothly. There were a few slow moments, but nothing that looked serious or out of the ordinary. If anything changes, I‚Äôll take another look and make sure it‚Äôs all sorted.\"\n",
    "]\n",
    "\n",
    "# Ground truth references\n",
    "references = [\n",
    "    \"The system collects raw input data and converts it into a standardized format for processing. A central module then applies predefined rules to analyze the data and generate intermediate results. Finally, the output component compiles these results into a structured report that can be used by downstream applications.\",\n",
    "    \"The application monitors incoming signals and filters out any values that fall outside the expected range. For example, if the signal is a random number between 0 and 1, then it is possible to check whether it is positive or negative. If it is positive, then it is possible to determine whether it is positive or negative. As the signal is sent over the Internet, it is possible to send a number of different data, such as a random number, to the processor\",\n",
    "    \"I checked the system this morning, and everything seemed ot be running smoothly. It‚Äôs a good time to do a little more research on the topic of ‚Äúhow to do something that makes sense I‚Äôm a bit of a writer, so I want to know how to get some of the ‚Äústuff‚Äù I‚Äôve been using in my head. So far I‚Äôve been using the\"\n",
    "]\n",
    "\n",
    "# Evaluate\n",
    "results = evaluate_predictions(predictions, references)\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"BERTScore Precision: {results['bertscore_precision']:.4f}\")\n",
    "print(f\"BERTScore Recall: {results['bertscore_recall']:.4f}\")\n",
    "print(f\"BERTScore F1: {results['bertscore_f1']:.4f}\")\n",
    "print(f\"ROUGE-1: {results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {results['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {results['rougeL']:.4f}\")\n",
    "print(f\"BLEU: {results['bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622b07b",
   "metadata": {},
   "source": [
    "# GRADIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d960484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Patrick\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\gradio\\interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated. Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio UI\n",
    "def generate_response(prompt, max_tokens=100, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Wrapper function for Gradio interface.\n",
    "    Calls generate_continuation without printing.\n",
    "    \"\"\"\n",
    "    return generate_continuation(\n",
    "        prompt, \n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        print_result=False\n",
    "    )\n",
    "\n",
    "# Create Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            lines=3, \n",
    "            placeholder=\"Enter your academic prompt here...\",\n",
    "            label=\"Input Prompt\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=20,\n",
    "            maximum=200,\n",
    "            value=100,\n",
    "            step=10,\n",
    "            label=\"Max Tokens\",\n",
    "            info=\"Maximum number of tokens to generate\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.5,\n",
    "            value=0.7,\n",
    "            step=0.1,\n",
    "            label=\"Temperature\",\n",
    "            info=\"Higher = more creative, Lower = more focused\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.0,\n",
    "            value=0.9,\n",
    "            step=0.05,\n",
    "            label=\"Top-p (Nucleus Sampling)\",\n",
    "            info=\"Controls diversity of output\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Textbox(\n",
    "        lines=5,\n",
    "        label=\"Generated Continuation\"\n",
    "    ),\n",
    "    title=\"üéì Academic Writing LoRA Model\",\n",
    "    description=\"Generate formal academic text continuations using a fine-tuned GPT-2 model with LoRA adapters.\",\n",
    "    examples=[\n",
    "        [\"Deep reinforcement learning has been widely adopted in robotic navigation.\", 100, 0.7, 0.9],\n",
    "        [\"The key findings of this research are\", 80, 0.7, 0.9],\n",
    "        [\"In recent years, machine learning has demonstrated\", 120, 0.8, 0.9],\n",
    "        [\"The methodology employed in this study involves\", 100, 0.6, 0.9],\n",
    "        [\"These results suggest that\", 90, 0.7, 0.9]\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch(share=False, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d38f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run Gradio, just execute the cell above. No need for a separate command."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
