{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe25ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio transformers accelerate peft --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c362e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jason\\anaconda3\\envs\\openai\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074303a",
   "metadata": {},
   "source": [
    "# Academic Writing LoRa Model: \n",
    "<br>\n",
    "<p>This Project creates an Academic Writing Assistant that generates text continuations for the user in an academic-style.</p>\n",
    "<p>\n",
    "The idea for this project started in code, CoPilot has been a massive success that has aided millions of people in coding. Providing real time assistance, as well as an auto-complete function for coding based on previous works of the user. This, significantly improving the experience of users, has not been applied to a wide range in natural language. </p>\n",
    "<p>\n",
    "While autocomplete has been around for a while, one area wherein text continuation has not been taken advantage of is in academic writing -- As Copilot has provided a large quality of life experience for those in coding, this project aims to do the same for academic writers. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50b2c2",
   "metadata": {},
   "source": [
    "# Why Lora? \n",
    "<p>\n",
    "Lora is a great fit for the initial task of training and fine tuning the model as the original task of the code(text continuation) can already be done on a surface level. \n",
    "</p>\n",
    "<p>\n",
    "The main adaptation of this code is to create a text continuation for academic writing -- That is, training a model for the specific task of writing for academic writings and scholarly papers. In the context of this code, this means that Lora is perfect for us as it allows us to fine tune an already proven and working base model (AutoModeforCausalM) and create our academic writing auto continuation without the exceedingly high computational costs of training an entire base model by ourselves.  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a0f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c28a29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jason\\anaconda3\\envs\\OpenAI\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "import gradio as gr\n",
    "\n",
    "# Load LoRA model using the approach from model_evaluation_and_finetuning.ipynb\n",
    "model_name = \"./lora_academic_model\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "model = PeftModel.from_pretrained(base_model, model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Add a padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7b775",
   "metadata": {},
   "source": [
    "### TESTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd04ff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Deep reinforcement learning has been widely adopted in robotic navigation.\n",
      "\n",
      "Generated Continuation: The method can be implemented in a number of different ways, including linear, non-linear, and adaptive. The linear method is widely used in the medical and robotic navigation. The non-linear method is widely used in the medical and robotic navigation.\n",
      "In the linear method, the position of a robot is determined by the robot's angular velocity, and the position of a robot is determined by the position of a robot's eyes. The position of a robot is a point at which the robot's\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Ensure the fine-tuned model and tokenizer are loaded (from cell AlY-JVt2g7M6)\n",
    "# If they are not in scope, you might need to run cell AlY-JVt2g7M6 first.\n",
    "\n",
    "# Define a context for generation\n",
    "# Using the same context as the stylistic control example\n",
    "context = \"Deep reinforcement learning has been widely adopted in robotic navigation.\"\n",
    "\n",
    "# Tokenize the input context\n",
    "inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate continuation using the fine-tuned model\n",
    "# You can adjust max_new_tokens, do_sample, temperature, top_p as needed\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask, # Explicitly pass attention_mask\n",
    "    max_new_tokens=100, # Generate up to 100 new tokens\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,      # Enable sampling for more diverse outputs\n",
    "    temperature=0.7,     # Control creativity (lower for less, higher for more)\n",
    "    top_p=0.9,           # Nucleus sampling\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the generated tokens, skipping the input context and special tokens\n",
    "generated_text = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"Context:\", context)\n",
    "print(\"\\nGenerated Continuation:\", generated_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10424a7d",
   "metadata": {},
   "source": [
    "### FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b32b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LoRA model with sample prompts:\n",
      "\n",
      "Context: Deep reinforcement learning has been widely adopted in robotic navigation.\n",
      "\n",
      "Generated Continuation: For example, in the case of robotic navigation, it is possible to learn the direction of the robot using the motion of the robot (e.g., the robot will move in the direction of the robot). However, it is not possible to learn the direction of the robot using the motion of the robot.\n",
      "In addition, there is a need to combine the motion of the robot and the motion\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Context: The key findings of this research are\n",
      "\n",
      "Generated Continuation: as follows: (1) The mean age of the patients is 65.5 years, (2) the mean age of the patients is 81.7 years, and (3) the mean age of the patients is 80.8 years, which are similar to those in previous studies. (4) The mean age of the patients is 80.7 years, and (5) the mean age\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Context: In recent years, machine learning has demonstrated\n",
      "\n",
      "Generated Continuation: its ability to model and control the spread of infectious diseases. However, the use of machine learning has not been recognized for the most part. The use of machine learning to predict disease spread is not limited to the diseases, but to the disease itself. The main purpose of machine learning is to predict disease spread and the probability of disease spread.\n",
      "For example, the method of prediction of the spread of\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_continuation(context, max_new_tokens=100, temperature=0.7, top_p=0.9, print_result=True):\n",
    "    \"\"\"\n",
    "    Generate academic text continuation using the loaded LoRA model.\n",
    "    \n",
    "    Args:\n",
    "        context (str): Input prompt/context for generation\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate (default: 100)\n",
    "        temperature (float): Sampling temperature - higher for more creativity (default: 0.7)\n",
    "        top_p (float): Nucleus sampling parameter (default: 0.9)\n",
    "        print_result (bool): Whether to print the input and output (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated continuation text\n",
    "    \"\"\"\n",
    "    # Tokenize the input context\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate continuation using the fine-tuned model\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens, skipping the input context and special tokens\n",
    "    generated_text = tokenizer.decode(\n",
    "        output_ids[0][inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    if print_result:\n",
    "        print(\"Context:\", context)\n",
    "        print(\"\\nGenerated Continuation:\", generated_text.strip())\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "\n",
    "# Test the function with sample prompts\n",
    "test_prompts = [\n",
    "    \"Deep reinforcement learning has been widely adopted in robotic navigation.\",\n",
    "    \"The key findings of this research are\",\n",
    "    \"In recent years, machine learning has demonstrated\"\n",
    "]\n",
    "\n",
    "print(\"Testing LoRA model with sample prompts:\\n\")\n",
    "for prompt in test_prompts:\n",
    "    generate_continuation(prompt, max_new_tokens=80)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622b07b",
   "metadata": {},
   "source": [
    "# GRADIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d960484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jason\\anaconda3\\envs\\OpenAI\\Lib\\site-packages\\gradio\\interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio UI\n",
    "def generate_response(prompt, max_tokens=100, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Wrapper function for Gradio interface.\n",
    "    Calls generate_continuation without printing.\n",
    "    \"\"\"\n",
    "    return generate_continuation(\n",
    "        prompt, \n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        print_result=False\n",
    "    )\n",
    "\n",
    "# Create Gradio interface\n",
    "interface = gr.Interface(\n",
    "    fn=generate_response,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            lines=3, \n",
    "            placeholder=\"Enter your academic prompt here...\",\n",
    "            label=\"Input Prompt\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=20,\n",
    "            maximum=200,\n",
    "            value=100,\n",
    "            step=10,\n",
    "            label=\"Max Tokens\",\n",
    "            info=\"Maximum number of tokens to generate\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.5,\n",
    "            value=0.7,\n",
    "            step=0.1,\n",
    "            label=\"Temperature\",\n",
    "            info=\"Higher = more creative, Lower = more focused\"\n",
    "        ),\n",
    "        gr.Slider(\n",
    "            minimum=0.1,\n",
    "            maximum=1.0,\n",
    "            value=0.9,\n",
    "            step=0.05,\n",
    "            label=\"Top-p (Nucleus Sampling)\",\n",
    "            info=\"Controls diversity of output\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Textbox(\n",
    "        lines=5,\n",
    "        label=\"Generated Continuation\"\n",
    "    ),\n",
    "    title=\"ðŸŽ“ Academic Writing LoRA Model\",\n",
    "    description=\"Generate formal academic text continuations using a fine-tuned GPT-2 model with LoRA adapters.\",\n",
    "    examples=[\n",
    "        [\"Deep reinforcement learning has been widely adopted in robotic navigation.\", 100, 0.7, 0.9],\n",
    "        [\"The key findings of this research are\", 80, 0.7, 0.9],\n",
    "        [\"In recent years, machine learning has demonstrated\", 120, 0.8, 0.9],\n",
    "        [\"The methodology employed in this study involves\", 100, 0.6, 0.9],\n",
    "        [\"These results suggest that\", 90, 0.7, 0.9]\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch(share=False, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d38f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run Gradio, just execute the cell above. No need for a separate command."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
