{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be0cc44",
   "metadata": {},
   "source": [
    "# Academic Writter Assistant — Notebook\n",
    "\n",
    "## NLP GROUP PROJECT\n",
    "\n",
    "**Purpose**: End-to-end notebook for building an autocomplete assistant that predicts next-sentence continuations from paragraph context (100–200 tokens). This notebook contains dataset templates, NSP evaluation approaches, fine-tuning recipe, re-ranking head design, context-window extension ideas, stylistic control methods, vocabulary/token distribution analysis, and evaluation guidance.\n",
    "\n",
    "**Run notes**: Install `transformers`, `datasets`, `accelerate`, and other libraries in your runtime before executing heavy training cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9fe5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers datasets accelerate evaluate sentencepiece tokenizers faiss-cpu evaluate nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e010808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json, math, random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from typing import List, Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertForNextSentencePrediction, TrainingArguments, Trainer\n",
    "import nltk\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Separating helper functions to declutter the code\n",
    "import dataset_utils\n",
    "from model_utils import generate_continuations, all_model_evaluation, nsp_score, lm_score\n",
    "# Defining the device that the models run on.\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e7dc3",
   "metadata": {},
   "source": [
    "# Building a dataset\n",
    "Here, we create a dataset of contexts and text sentence, using a number of sources. The current implementation only retrieves text from ASAP Essays. Data is first cleaned then loaded into a dataframe with context and next sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e9be60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading ASAP Essays\n",
      "Total raw documents loaded: 723\n",
      "Generating (context, continuation) pairs...\n",
      "Generated 720 training pairs.\n",
      "                                             context  \\\n",
      "0  A long time ago when I was in third grade I ha...   \n",
      "1  Softball has to be one of the single most grea...   \n",
      "2  Some people like making people laugh, I love i...   \n",
      "3  \"LAUGHTER\" @CAPS1 I hang out with my friends, ...   \n",
      "4  Well ima tell a story about the time i got @CA...   \n",
      "\n",
      "                                        continuation  \n",
      "0  The next day @PERSON2 and I were eating lunch ...  \n",
      "1  Many of these girls were like sisters to me th...  \n",
      "2  For example one time I hit myself in the head ...  \n",
      "3  @CAPS1 I say trash can I really mean trash can...  \n",
      "4  Then she said stupid @CAPS2 on the bus and com...  \n",
      "\n",
      "A long time ago when I was in third grade I had a friend @PERSON2 who's mom was in a bad mood. She never laughed and she never smiled. Every time I saw her I would smile at her and all she would do was frown and keep walking. At first I didn't know she was a grouch i just thought she didn't like me or something.When @PERSON2 told me his mom was a grouch I started to laugh and laugh. He asked me what was so funny i told him that i thought his mom didn't like me or something because every time I see his mom I would smile at her and all she will do is frown and walk away. That made my friend laugh we were cracking up so hard that we got in trouble in class.\n"
     ]
    }
   ],
   "source": [
    "df = dataset_utils.build_academic_dataset(tokenizer=None, limit_each=3000)\n",
    "contexts = df['context']\n",
    "print(df.head())\n",
    "print()\n",
    "print(contexts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d22047f",
   "metadata": {},
   "source": [
    "# Testing next text generation\n",
    "Here, we load a GPT2 model to test text generation on the dataset that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25588ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e07bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Deep reinforcement learning has been widely adopted in robotic navigation.\n",
      "\n",
      "Generated Continuation: But it has its limits, especially when it comes to detecting objects.\n",
      "\n",
      "This article will show how to use a simple reinforcement learning algorithm to detect objects with a certain speed, and what it can do to help you improve your performance.\n",
      "\n",
      "The algorithm, known as an artificial intelligence (AI) or Deep Learning, is a neural network with the ability to learn to recognize objects, such as humans. It works by identifying the direction of an object's movement.\n",
      "\n",
      "An AI may be\n"
     ]
    }
   ],
   "source": [
    "# Generating text with the GPT-2 model\n",
    "context = \"Deep reinforcement learning has been widely adopted in robotic navigation.\"\n",
    "\n",
    "# Tokenize the input context\n",
    "inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate continuation using the fine-tuned model\n",
    "# You can adjust max_new_tokens, do_sample, temperature, top_p as needed\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask, # Explicitly pass attention_mask\n",
    "    max_new_tokens=100, # Generate up to 100 new tokens\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,      # Enable sampling for more diverse outputs\n",
    "    temperature=0.7,     # Control creativity (lower for less, higher for more)\n",
    "    top_p=0.9,           # Nucleus sampling\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the generated tokens, skipping the input context and special tokens\n",
    "generated_text = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"Context:\", context)\n",
    "print(\"\\nGenerated Continuation:\", generated_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949aff6",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Here we are going to test some model evaluation methods for next sentence prediction (NSP). \n",
    "\n",
    "## Language model scoring task\n",
    "\n",
    "This approach treats NSP as a language-model scoring task.\n",
    "\n",
    "Process:\n",
    "\n",
    "- Given a sentence pair (A, candidate continuation B)\n",
    "\n",
    "- Compute the log-likelihood or perplexity of B given A\n",
    "\n",
    "- Higher probability = more plausible continuation\n",
    "\n",
    "- Compare scores between true and false continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd6fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: -574.0296041965485\n"
     ]
    }
   ],
   "source": [
    "# Run the following code if you want to switch to distilgpt2\n",
    "\n",
    "# model_name = \"distilgpt2\"\n",
    "# lm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# lm_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# lm_model.to(device)\n",
    "# lm_model.eval()\n",
    "\n",
    "score = lm_score(model=model, \n",
    "\t\t\t\t tokenizer=tokenizer,\n",
    "\t\t\t\t context=contexts[0],\n",
    "\t\t\t\t continuation=df['continuation'][0],\n",
    "\t\t\t\t device=device)\n",
    "\n",
    "print(\"Classification score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496cb0a",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "This approach treats NSP as a binary classification task.\n",
    "\n",
    "Process:\n",
    "\n",
    "- Concatenate sentence pair (A + B)\n",
    "\n",
    "- Feed into a classifier (e.g., BERT, RoBERTa, DeBERTa)\n",
    "\n",
    "- Output:\n",
    "\n",
    "  - 1 → B is a valid continuation\n",
    "\n",
    "  - 0 → B is an invalid continuation\n",
    "\n",
    "- Evaluate using: Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36563d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: 0.9999972581863403\n"
     ]
    }
   ],
   "source": [
    "eval_model = \"bert-base-uncased\"\n",
    "eval_tokenizer = BertTokenizer.from_pretrained(eval_model)\n",
    "eval_model = BertForNextSentencePrediction.from_pretrained(eval_model)\n",
    "eval_model.eval()\n",
    "eval_model.to(device)\n",
    "\n",
    "score = nsp_score(bert_model=eval_model, \n",
    "\t\t  bert_tokenizer=eval_tokenizer, \n",
    "\t\t  context=contexts[0], \n",
    "\t\t  continuation=df['continuation'][0], \n",
    "\t\t  device=device)\n",
    "\n",
    "print(\"Classification score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e15977",
   "metadata": {},
   "source": [
    "# Model evaluation with Ground Truth\n",
    "\n",
    "- BERTScore **Semantic similarity**\n",
    "\n",
    "- ROUGE (ROUGE-1, ROUGE-2, ROUGE-L) **Lexical overlap**\n",
    "\n",
    "- BLEU **N-gram precision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35bf8265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tranh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tranh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "c:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_lm': -471.00285816192627,\n",
       " 'avg_nsp': 0.010369982570409775,\n",
       " 'avg_final': -282.58097493201495,\n",
       " 'bertscore_precision': np.float64(0.4200078547000885),\n",
       " 'bertscore_recall': np.float64(0.27861446142196655),\n",
       " 'bertscore_f1': np.float64(0.33500292897224426),\n",
       " 'rouge1': np.float64(0.02631578947368421),\n",
       " 'rouge2': np.float64(0.0),\n",
       " 'rougeL': np.float64(0.02631578947368421),\n",
       " 'bleu': 3.169663824442922e-242}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from model_utils import all_model_evaluation\n",
    "\n",
    "model = model.to(device)\n",
    "eval_model = eval_model.to(device)\n",
    "\n",
    "model_score = all_model_evaluation(\n",
    "\t\tmodel=model,\n",
    "\t\ttokenizer=tokenizer,\n",
    "\t\tbert_model=eval_model,\n",
    "\t\tbert_tokenizer=eval_tokenizer,\n",
    "        contexts=[contexts[0]],\n",
    "        candidates=[\"He is very good with it\"]\n",
    "    )\n",
    "model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186477df",
   "metadata": {},
   "source": [
    "# Comparing model\n",
    "Testing some smaller language models to see which one is the most suitable for our task. These models include:\n",
    "- gpt2\n",
    "- EleutherAI/gpt-neo-125M\n",
    "- facebook/opt-125m\n",
    "- microsoft/DialoGPT-small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05eea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating continuations for gpt2...\n"
     ]
    }
   ],
   "source": [
    "causal_models = [\n",
    "    \"gpt2\",\n",
    "    \"EleutherAI/gpt-neo-125M\",\n",
    "    \"facebook/opt-125m\",\n",
    "    \"microsoft/DialoGPT-small\"\n",
    "]\n",
    "run = False # Change this to run again\n",
    "if run:\n",
    "\tmodel.to(device)\n",
    "\teval_model.to(device)\n",
    "\n",
    "\tall_model_evaluation_score = []\n",
    "\tfor model_name in causal_models:\n",
    "\t\tprint(f\"Generating continuations for {model_name}...\")\n",
    "\t\t# Make sure 'gen_model' and 'gen_tokenizer' are available or reloaded if needed\n",
    "\t\t# (assuming they are from the first model generation cell)\n",
    "\t\tgenerate_continuations_list = generate_continuations(model_name, contexts, max_new_tokens=50, device=device)\n",
    "\t\tprint(f\"Generated {len(generate_continuations_list)} continuations.\")\n",
    "\t\tprint(generate_continuations_list[:5])\n",
    "\t\tmodel_score = all_model_evaluation(\n",
    "\t\t\tmodel=model,\n",
    "\t\t\ttokenizer=tokenizer,\n",
    "\t\t\tbert_model=eval_model,\n",
    "\t\t\tbert_tokenizer=eval_tokenizer,\n",
    "\t\t\tcontexts=contexts,\n",
    "\t\t\tcandidates=generate_continuations_list,\n",
    "\t\t\tdevice=device\n",
    "\t\t)\n",
    "\t\tall_model_evaluation_score.append(model_score)\n",
    "\n",
    "\tprint(all_model_evaluation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c1b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_scores = pd.DataFrame(all_model_evaluation_score)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_scores.to_csv('model_evaluation_scores.csv', index=False)\n",
    "\n",
    "print('Model evaluation scores saved to model_evaluation_scores.csv')\n",
    "\n",
    "# Display the DataFrame to show the saved data\n",
    "display(df_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20562a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Ensure `df_scores` and `causal_models` are available\n",
    "# If `causal_models` is not updated or might be out of sync with df_scores rows,\n",
    "# we might need to recreate it or infer model names.\n",
    "# For now, assuming causal_models is still the list of models that generated the scores.\n",
    "\n",
    "# Adding model names to the DataFrame for easier plotting\n",
    "model_names = causal_models # Use the last set of causal_models that was evaluated\n",
    "df_scores['model_name'] = model_names[:len(df_scores)] # Ensure lengths match\n",
    "\n",
    "metrics_to_plot = [\n",
    "    'bertscore_f1',\n",
    "    'rougeL',\n",
    "    'bleu',\n",
    "    'avg_final'\n",
    "]\n",
    "\n",
    "metric_titles = {\n",
    "    'bertscore_f1': 'BERTScore F1',\n",
    "    'rougeL': 'ROUGE-L',\n",
    "    'bleu': 'BLEU Score',\n",
    "    'avg_final': 'Average Hybrid Score'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    sns.barplot(x='model_name', y=metric, data=df_scores, ax=axes[i], palette='viridis', hue='model_name', legend=False)\n",
    "    axes[i].set_title(metric_titles[metric])\n",
    "    axes[i].set_xlabel('Model Name')\n",
    "    axes[i].set_ylabel('Score')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_evaluation_metrics.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Evaluation metrics plots saved to model_evaluation_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f506fa",
   "metadata": {},
   "source": [
    "## Choosing Best Pretrained-Model\n",
    "\n",
    "From the Result we can conclude that Overall Best Model: → GPT-Neo 125M (Model 1)\n",
    "\n",
    "It gives the best balance between likelihood scoring, semantic understanding, lexical overlap, and NSP performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
