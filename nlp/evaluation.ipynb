{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be0cc44",
   "metadata": {},
   "source": [
    "# Academic Writter Assistant — Notebook\n",
    "\n",
    "## NLP GROUP PROJECT\n",
    "\n",
    "**Purpose**: End-to-end notebook for building an autocomplete assistant that predicts next-sentence continuations from paragraph context (100–200 tokens). This notebook contains dataset templates, NSP evaluation approaches, fine-tuning recipe, re-ranking head design, context-window extension ideas, stylistic control methods, vocabulary/token distribution analysis, and evaluation guidance.\n",
    "\n",
    "**Run notes**: Install `transformers`, `datasets`, `accelerate`, and other libraries in your runtime before executing heavy training cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9fe5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers datasets accelerate evaluate sentencepiece tokenizers faiss-cpu evaluate nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e010808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json, math, random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "from typing import List, Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertForNextSentencePrediction, TrainingArguments, Trainer\n",
    "import nltk\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Separating helper functions to declutter the code\n",
    "import dataset_utils\n",
    "\n",
    "# Defining the device that the models run on.\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032e7dc3",
   "metadata": {},
   "source": [
    "# Building a dataset\n",
    "Here, we create a dataset of contexts and text sentence, using a number of sources. The current implementation only retrieves text from ASAP Essays. Data is first cleaned then loaded into a dataframe with context and next sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e9be60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading ASAP Essays\n",
      "Total raw documents loaded: 723\n",
      "Generating (context, continuation) pairs...\n",
      "Generated 720 training pairs.\n",
      "                                             context  \\\n",
      "0  A long time ago when I was in third grade I ha...   \n",
      "1  Softball has to be one of the single most grea...   \n",
      "2  Some people like making people laugh, I love i...   \n",
      "3  \"LAUGHTER\" @CAPS1 I hang out with my friends, ...   \n",
      "4  Well ima tell a story about the time i got @CA...   \n",
      "\n",
      "                                        continuation  \n",
      "0  The next day @PERSON2 and I were eating lunch ...  \n",
      "1  Many of these girls were like sisters to me th...  \n",
      "2  For example one time I hit myself in the head ...  \n",
      "3  @CAPS1 I say trash can I really mean trash can...  \n",
      "4  Then she said stupid @CAPS2 on the bus and com...  \n",
      "\n",
      "A long time ago when I was in third grade I had a friend @PERSON2 who's mom was in a bad mood. She never laughed and she never smiled. Every time I saw her I would smile at her and all she would do was frown and keep walking. At first I didn't know she was a grouch i just thought she didn't like me or something.When @PERSON2 told me his mom was a grouch I started to laugh and laugh. He asked me what was so funny i told him that i thought his mom didn't like me or something because every time I see his mom I would smile at her and all she will do is frown and walk away. That made my friend laugh we were cracking up so hard that we got in trouble in class.\n"
     ]
    }
   ],
   "source": [
    "df = dataset_utils.build_academic_dataset(tokenizer=None, limit_each=3000)\n",
    "contexts = df['context']\n",
    "print(df.head())\n",
    "print()\n",
    "print(contexts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d22047f",
   "metadata": {},
   "source": [
    "# Testing next text generation\n",
    "Here, we load a GPT2 model to test text generation on the dataset that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25588ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model_utils\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51e07bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Deep reinforcement learning has been widely adopted in robotic navigation.\n",
      "\n",
      "Generated Continuation: However, there is a limit to how well it can be applied to artificial intelligence.\n",
      "\n",
      "This week, we analyzed data from the world's largest robot park in California, the San Diego Zoo.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "In this study, we used a robotic robotic arm to measure the effectiveness of reinforcement learning in a range of tasks. This arm was trained to take a series of steps to complete a task, then trained to move the robotic arm forward. We found that the robot arm's performance\n"
     ]
    }
   ],
   "source": [
    "# Generating text with the GPT-2 model\n",
    "context = \"Deep reinforcement learning has been widely adopted in robotic navigation.\"\n",
    "\n",
    "# Tokenize the input context\n",
    "inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate continuation using the fine-tuned model\n",
    "# You can adjust max_new_tokens, do_sample, temperature, top_p as needed\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask, # Explicitly pass attention_mask\n",
    "    max_new_tokens=100, # Generate up to 100 new tokens\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,      # Enable sampling for more diverse outputs\n",
    "    temperature=0.7,     # Control creativity (lower for less, higher for more)\n",
    "    top_p=0.9,           # Nucleus sampling\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the generated tokens, skipping the input context and special tokens\n",
    "generated_text = tokenizer.decode(output_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "print(\"Context:\", context)\n",
    "print(\"\\nGenerated Continuation:\", generated_text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d949aff6",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Here we are going to test some model evaluation methods for next sentence prediction (NSP). \n",
    "\n",
    "## Language model scoring task\n",
    "\n",
    "This approach treats NSP as a language-model scoring task.\n",
    "\n",
    "Process:\n",
    "\n",
    "- Given a sentence pair (A, candidate continuation B)\n",
    "\n",
    "- Compute the log-likelihood or perplexity of B given A\n",
    "\n",
    "- Higher probability = more plausible continuation\n",
    "\n",
    "- Compare scores between true and false continuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fd6fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: -574.0275120735168\n"
     ]
    }
   ],
   "source": [
    "# Run the following code if you want to switch to distilgpt2\n",
    "\n",
    "# model_name = \"distilgpt2\"\n",
    "# lm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# lm_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# lm_model.to(device)\n",
    "# lm_model.eval()\n",
    "\n",
    "score = model_utils.lm_score(model=model, \n",
    "\t\t\t\t tokenizer=tokenizer,\n",
    "\t\t\t\t context=contexts[0],\n",
    "\t\t\t\t continuation=df['continuation'][0],\n",
    "\t\t\t\t device=\"cpu\")\n",
    "\n",
    "print(\"Classification score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496cb0a",
   "metadata": {},
   "source": [
    "## Binary classification\n",
    "\n",
    "This approach treats NSP as a binary classification task.\n",
    "\n",
    "Process:\n",
    "\n",
    "- Concatenate sentence pair (A + B)\n",
    "\n",
    "- Feed into a classifier (e.g., BERT, RoBERTa, DeBERTa)\n",
    "\n",
    "- Output:\n",
    "\n",
    "  - 1 → B is a valid continuation\n",
    "\n",
    "  - 0 → B is an invalid continuation\n",
    "\n",
    "- Evaluate using: Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d36563d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification score: 0.9999972581863403\n"
     ]
    }
   ],
   "source": [
    "eval_model = \"bert-base-uncased\"\n",
    "eval_tokenizer = BertTokenizer.from_pretrained(eval_model)\n",
    "eval_model = BertForNextSentencePrediction.from_pretrained(eval_model)\n",
    "eval_model.eval()\n",
    "eval_model.to(device)\n",
    "\n",
    "from model_utils import nsp_score\n",
    "\n",
    "score = nsp_score(bert_model=eval_model, \n",
    "\t\t  bert_tokenizer=eval_tokenizer, \n",
    "\t\t  context=contexts[0], \n",
    "\t\t  continuation=df['continuation'][0], \n",
    "\t\t  device=device)\n",
    "\n",
    "print(\"Classification score:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e15977",
   "metadata": {},
   "source": [
    "# Model evaluation with Ground Truth\n",
    "\n",
    "- BERTScore **Semantic similarity**\n",
    "\n",
    "- ROUGE (ROUGE-1, ROUGE-2, ROUGE-L) **Lexical overlap**\n",
    "\n",
    "- BLEU **N-gram precision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35bf8265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tranh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tranh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "c:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_lm': -471.00285816192627,\n",
       " 'avg_nsp': 0.010369982570409775,\n",
       " 'avg_final': -282.58097493201495,\n",
       " 'bertscore_precision': np.float64(0.42000794410705566),\n",
       " 'bertscore_recall': np.float64(0.27861449122428894),\n",
       " 'bertscore_f1': np.float64(0.33500295877456665),\n",
       " 'rouge1': np.float64(0.02631578947368421),\n",
       " 'rouge2': np.float64(0.0),\n",
       " 'rougeL': np.float64(0.02631578947368421),\n",
       " 'bleu': 3.169663824442922e-242}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model_utils import evaluate_predictions, all_model_evaluation\n",
    "\n",
    "model_score = all_model_evaluation(\n",
    "\t\tmodel=model,\n",
    "\t\ttokenizer=tokenizer,\n",
    "\t\tbert_model=eval_model,\n",
    "\t\tbert_tokenizer=eval_tokenizer,\n",
    "        contexts=[contexts[0]],\n",
    "        candidates=[\"He is very good with it\"]\n",
    "    )\n",
    "model_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186477df",
   "metadata": {},
   "source": [
    "# Comparing model\n",
    "Testing some smaller language models to see which one is the most suitable for our task. These models include:\n",
    "- gpt2\n",
    "- EleutherAI/gpt-neo-125M\n",
    "- facebook/opt-125m\n",
    "- microsoft/DialoGPT-small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a05eea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating continuations for gpt2...\n",
      "Warning: Empty input_ids for context: ''. Skipping generation.\n",
      "Warning: Empty input_ids for context: ''. Skipping generation.\n",
      "Generated 720 continuations.\n",
      "[\"I was still young then and I was really depressed. I couldn't believe how much he was trying to make me laugh and so bad I couldn't stop laughing. I told my mom I wouldn't laugh at her because she was trying to make me\", 'I was very excited to get into softball. I loved playing with my dad, and my brother and I, and the team I grew up with. We were the only team that went to a team game in the 80s (that was then', 'I always wanted to be a spaz, I think I was a kid. I love to play with my toys. I love to play with my friends. I love to watch the world burn. I love to love to make people laugh. So', 'I am also a big fan of the way they talk to each other. We get along pretty well when it comes to their personality. They are all very friendly and easygoing. They talk about the things that they do for a living, and we', 'She was like are ugh. So i went back and said were you at then i said no no i was at then i said i was. Then i went home to do a lot of stuff and all of a sudden i thought ugh.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tranh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tranh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating continuations for EleutherAI/gpt-neo-125M...\n",
      "Warning: Empty input_ids for context: ''. Skipping generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerating continuations for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Make sure 'gen_model' and 'gen_tokenizer' are available or reloaded if needed\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# (assuming they are from the first model generation cell)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m generate_continuations_list = \u001b[43mmodel_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_continuations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(generate_continuations_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m continuations.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(generate_continuations_list[:\u001b[32m5\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Projects\\nlp-group\\nlp\\model_utils.py:45\u001b[39m, in \u001b[36mgenerate_continuations\u001b[39m\u001b[34m(model_name, contexts, max_new_tokens, device)\u001b[39m\n\u001b[32m     42\u001b[39m \t\u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Generate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m outputs = \u001b[43mgen_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# allow variation, can set False for deterministic\u001b[39;49;00m\n\u001b[32m     49\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# creativity\u001b[39;49;00m\n\u001b[32m     50\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# nucleus sampling\u001b[39;49;00m\n\u001b[32m     51\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Extract generated continuation (skip input tokens)\u001b[39;00m\n\u001b[32m     55\u001b[39m continuation = gen_tokenizer.decode(\n\u001b[32m     56\u001b[39m \toutputs[\u001b[32m0\u001b[39m][inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].shape[\u001b[32m1\u001b[39m]:],\n\u001b[32m     57\u001b[39m \tskip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     58\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\transformers\\generation\\utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\transformers\\models\\gpt_neo\\modeling_gpt_neo.py:840\u001b[39m, in \u001b[36mGPTNeoForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[39m\n\u001b[32m    824\u001b[39m transformer_outputs = \u001b[38;5;28mself\u001b[39m.transformer(\n\u001b[32m    825\u001b[39m     input_ids,\n\u001b[32m    826\u001b[39m     past_key_values=past_key_values,\n\u001b[32m   (...)\u001b[39m\u001b[32m    836\u001b[39m     cache_position=cache_position,\n\u001b[32m    837\u001b[39m )\n\u001b[32m    838\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m lm_logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    842\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    844\u001b[39m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tranh\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "causal_models = [\n",
    "    \"gpt2\",\n",
    "    \"EleutherAI/gpt-neo-125M\",\n",
    "    \"facebook/opt-125m\",\n",
    "    \"microsoft/DialoGPT-small\"\n",
    "]\n",
    "\n",
    "model.to(device)\n",
    "eval_model.to(device)\n",
    "\n",
    "all_model_evaluation_score = []\n",
    "for model_name in causal_models:\n",
    "    print(f\"Generating continuations for {model_name}...\")\n",
    "    # Make sure 'gen_model' and 'gen_tokenizer' are available or reloaded if needed\n",
    "    # (assuming they are from the first model generation cell)\n",
    "    generate_continuations_list = model_utils.generate_continuations(model_name, contexts, max_new_tokens=50, device=device)\n",
    "    print(f\"Generated {len(generate_continuations_list)} continuations.\")\n",
    "    print(generate_continuations_list[:5])\n",
    "    model_score = all_model_evaluation(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        bert_model=eval_model,\n",
    "        bert_tokenizer=eval_tokenizer,\n",
    "        contexts=contexts,\n",
    "        candidates=generate_continuations_list,\n",
    "        device=device\n",
    "    )\n",
    "    all_model_evaluation_score.append(model_score)\n",
    "\n",
    "print(all_model_evaluation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c1b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df_scores = pd.DataFrame(all_model_evaluation_score)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_scores.to_csv('model_evaluation_scores.csv', index=False)\n",
    "\n",
    "print('Model evaluation scores saved to model_evaluation_scores.csv')\n",
    "\n",
    "# Display the DataFrame to show the saved data\n",
    "display(df_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20562a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Ensure `df_scores` and `causal_models` are available\n",
    "# If `causal_models` is not updated or might be out of sync with df_scores rows,\n",
    "# we might need to recreate it or infer model names.\n",
    "# For now, assuming causal_models is still the list of models that generated the scores.\n",
    "\n",
    "# Adding model names to the DataFrame for easier plotting\n",
    "model_names = causal_models # Use the last set of causal_models that was evaluated\n",
    "df_scores['model_name'] = model_names[:len(df_scores)] # Ensure lengths match\n",
    "\n",
    "metrics_to_plot = [\n",
    "    'bertscore_f1',\n",
    "    'rougeL',\n",
    "    'bleu',\n",
    "    'avg_final'\n",
    "]\n",
    "\n",
    "metric_titles = {\n",
    "    'bertscore_f1': 'BERTScore F1',\n",
    "    'rougeL': 'ROUGE-L',\n",
    "    'bleu': 'BLEU Score',\n",
    "    'avg_final': 'Average Hybrid Score'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    sns.barplot(x='model_name', y=metric, data=df_scores, ax=axes[i], palette='viridis', hue='model_name', legend=False)\n",
    "    axes[i].set_title(metric_titles[metric])\n",
    "    axes[i].set_xlabel('Model Name')\n",
    "    axes[i].set_ylabel('Score')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_evaluation_metrics.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"Evaluation metrics plots saved to model_evaluation_metrics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f506fa",
   "metadata": {},
   "source": [
    "## Choosing Best Pretrained-Model\n",
    "\n",
    "From the Result we can conclude that Overall Best Model: → GPT-Neo 125M (Model 1)\n",
    "\n",
    "It gives the best balance between likelihood scoring, semantic understanding, lexical overlap, and NSP performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
