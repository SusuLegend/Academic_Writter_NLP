{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792de9a4",
   "metadata": {},
   "source": [
    "# Part 2: Finetuning\n",
    "From the evaluation scripts, we now know that GPT-Neo 125M is the best one for our purpose. Next we will try to finetune it to create a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee12288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import nltk\n",
    "import random\n",
    "device = \"cuda\"\n",
    "\n",
    "import dataset_utils\n",
    "import model_utils\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc269947",
   "metadata": {},
   "source": [
    "# Loading the model, tokenizer and dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd4f1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Loading ASAP Essays\n",
      "Total raw documents loaded: 300\n",
      "Generating (context, continuation) pairs...\n",
      "Generated 299 training pairs.\n",
      "                                             context  \\\n",
      "0  A long time ago when I was in third grade I ha...   \n",
      "1  Softball has to be one of the single most grea...   \n",
      "2  Some people like making people laugh, I love i...   \n",
      "3  \"LAUGHTER\" @CAPS1 I hang out with my friends, ...   \n",
      "4  Well ima tell a story about the time i got @CA...   \n",
      "\n",
      "                                        continuation  \n",
      "0  The next day @PERSON2 and I were eating lunch ...  \n",
      "1  Many of these girls were like sisters to me th...  \n",
      "2  For example one time I hit myself in the head ...  \n",
      "3  @CAPS1 I say trash can I really mean trash can...  \n",
      "4  Then she said stupid @CAPS2 on the bus and com...  \n"
     ]
    }
   ],
   "source": [
    "df = dataset_utils.build_academic_dataset(tokenizer=None, limit_each=300)\n",
    "contexts = df['context']\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c584818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18da0200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 126,083,328 || trainable%: 0.7017\n"
     ]
    }
   ],
   "source": [
    "def build_dataset_for_causal(pairs, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    Build token-level dataset for causal LM fine-tuning.\n",
    "\n",
    "    pairs: dict with keys \"context\", \"continuation\"\n",
    "    tokenizer: HuggingFace tokenizer\n",
    "    max_length: max token length\n",
    "\n",
    "    Returns: dict suitable for Dataset.from_dict()\n",
    "    \"\"\"\n",
    "    # Add pad token if missing\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Ensure EOS token exists\n",
    "    eos = tokenizer.eos_token if tokenizer.eos_token else \"</s>\"\n",
    "\n",
    "    # Merge context + continuation\n",
    "    texts = [\n",
    "        context.strip() + eos + \" \" + continuation.strip()\n",
    "        for context, continuation in zip(pairs[\"context\"], pairs[\"continuation\"])\n",
    "    ]\n",
    "\n",
    "    # Tokenize\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    # For causal LM, labels = input_ids (shifted inside model)\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
    "\n",
    "    return encodings\n",
    "\n",
    "tokenized_pairs = build_dataset_for_causal(df, tokenizer, max_length=512)\n",
    "dataset = Dataset.from_dict(tokenized_pairs)\n",
    "\n",
    "# Train / Validation split\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds = dataset[\"test\"]\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 4. Configure LoRA\n",
    "# --------------------------------------------------------\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],  # Updated for OPT models\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c1175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"./lora_academic_model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    eval_strategy=\"steps\", # Changed from evaluation_strategy to eval_strategy\n",
    "    logging_steps=20,\n",
    "    eval_steps=100,\n",
    "    save_steps=200,\n",
    "    num_train_epochs=10,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "def data_collator(features):\n",
    "    # `features` is a list of dictionaries, where each dictionary is one example from the dataset.\n",
    "    # We need to extract the 'input_ids', 'attention_mask', and 'labels' from each example\n",
    "    # and then stack them into tensors for the batch.\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([f[\"input_ids\"] for f in features], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor([f[\"attention_mask\"] for f in features], dtype=torch.long),\n",
    "        \"labels\": torch.tensor([f[\"labels\"] for f in features], dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4475cc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tranh\\AppData\\Local\\Temp\\ipykernel_12696\\2967949319.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 02:46, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.474600</td>\n",
       "      <td>1.292113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=170, training_loss=2.8909198143902946, metrics={'train_runtime': 167.9576, 'train_samples_per_second': 16.016, 'train_steps_per_second': 1.012, 'total_flos': 709958267043840.0, 'train_loss': 2.8909198143902946, 'epoch': 10.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer, # Pass the tokenizer to the Trainer for saving\n",
    "    data_collator=data_collator, # Pass the data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01fcb827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./lora_academic_model\\\\tokenizer_config.json',\n",
       " './lora_academic_model\\\\special_tokens_map.json',\n",
       " './lora_academic_model\\\\vocab.json',\n",
       " './lora_academic_model\\\\merges.txt',\n",
       " './lora_academic_model\\\\added_tokens.json',\n",
       " './lora_academic_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save LoRA weights\n",
    "trainer.save_model(\"./lora_academic_model\")\n",
    "model.save_pretrained(\"./lora_academic_model\")\n",
    "tokenizer.save_pretrained(\"./lora_academic_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d88b6",
   "metadata": {},
   "source": [
    "# Loading the model\n",
    "\n",
    "Testing loading the model to see if it works as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7bc47d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoForCausalLM(\n",
       "      (transformer): GPTNeoModel(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(2048, 768)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPTNeoBlock(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPTNeoAttention(\n",
       "              (attention): GPTNeoSelfAttention(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=768, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=768, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPTNeoMLP(\n",
       "              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"./lora_academic_model\"\n",
    "\n",
    "# 1 Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# GPT2 requires pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = \"cuda\"\n",
    "# 2 Load base model (must match the one you fine-tuned)\n",
    "# Use the same model_name selected for fine-tuning\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "print(f\"Loading model on device: {device}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.float16 if device == 'cuda' else torch.float32\n",
    ").to(device)\n",
    "\n",
    "# 3┒┓ Load LoRA adapters\n",
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model loaded on: {next(model.parameters()).device}\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f3fd82",
   "metadata": {},
   "source": [
    "# Modified architecture: Re-ranking head.\n",
    "Train a lightweight MLP scoring head on pooled LM representations. At inference, generate N candidates and re-rank using this head for better top-1 precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b28316",
   "metadata": {},
   "source": [
    "### 1: Creating a dataset with ML ArXiv Paper Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b6d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tranh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tranh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'candidates', 'labels'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "contexts = []\n",
    "positives = []\n",
    "\n",
    "NSPds = load_dataset(\"CShorten/ML-ArXiv-Papers\")\n",
    "\n",
    "for row in NSPds[\"train\"]:\n",
    "    sentences = nltk.sent_tokenize(row[\"abstract\"])\n",
    "    for i in range(len(sentences) - 1):\n",
    "        contexts.append(sentences[i])\n",
    "        positives.append(sentences[i+1])\n",
    "\n",
    "# Build a global pool of sentences\n",
    "sentence_pool = []\n",
    "for row in NSPds[\"train\"]:\n",
    "    sentence_pool.extend(nltk.sent_tokenize(row[\"abstract\"]))\n",
    "\n",
    "def sample_negatives(k):\n",
    "    return random.sample(sentence_pool, k)\n",
    "\n",
    "nsp_data = {\n",
    "    \"context\": [],\n",
    "    \"candidates\": [],\n",
    "    \"labels\": []\n",
    "}\n",
    "\n",
    "num_negatives = 3  # N candidates = 1 positive + 3 negatives\n",
    "sample_size = 100 # Increased from 10 to ensure sufficient training data\n",
    "\n",
    "# Ensure contexts and positives are of the same length\n",
    "assert len(contexts) == len(positives), \"Contexts and positives lists must have the same length.\"\n",
    "\n",
    "# Generate random indices for sampling\n",
    "# Use min(sample_size, len(contexts)) to avoid errors if contexts has less than 2000 items\n",
    "random_indices = random.sample(range(len(contexts)), min(sample_size, len(contexts)))\n",
    "\n",
    "for i in random_indices:\n",
    "    ctx = contexts[i]\n",
    "    pos = positives[i]\n",
    "    negs = sample_negatives(num_negatives)\n",
    "\n",
    "    nsp_data[\"context\"].append(ctx)\n",
    "    nsp_data[\"candidates\"].append([pos] + negs)\n",
    "    nsp_data[\"labels\"].append([1] + [0]*num_negatives)\n",
    "    \n",
    "rerank_ds = Dataset.from_dict(nsp_data)\n",
    "rerank_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc800c",
   "metadata": {},
   "source": [
    "## 2. Reranker Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48d0772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RerankerModel(nn.Module):\n",
    "    def __init__(self, lm, tokenizer, hidden_dim=768):\n",
    "        super().__init__()\n",
    "        self.lm = lm\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Ensure LM returns hidden states\n",
    "        self.lm.config.output_hidden_states = True\n",
    "        self.lm.config.return_dict = True\n",
    "\n",
    "        # MLP scoring head (FP32)\n",
    "        self.scorer = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.lm(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        hidden_states = outputs.hidden_states[-1]  # (B, T, H) in FP16\n",
    "\n",
    "        # find EOS\n",
    "        eos_mask = (input_ids == self.tokenizer.eos_token_id)\n",
    "        idx = eos_mask.float().argmax(dim=1)\n",
    "\n",
    "        # gather\n",
    "        batch_idx = torch.arange(hidden_states.size(0), device=hidden_states.device)\n",
    "        eos_hidden = hidden_states[batch_idx, idx]\n",
    "\n",
    "        # convert to float32 before MLP\n",
    "        eos_hidden = eos_hidden.float()\n",
    "\n",
    "        # scalar score\n",
    "        score = self.scorer(eos_hidden)  # (B, 1)\n",
    "        return score.squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9693024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class RerankDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_len=256):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        context = item[\"context\"]\n",
    "        candidates = item[\"candidates\"]\n",
    "        labels = item[\"labels\"]\n",
    "\n",
    "        input_ids_list = []\n",
    "        attn_list = []\n",
    "\n",
    "        for cand in candidates:\n",
    "            text = context + self.tokenizer.eos_token + \" \" + cand\n",
    "            enc = self.tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_ids_list.append(enc[\"input_ids\"])\n",
    "            attn_list.append(enc[\"attention_mask\"])\n",
    "\n",
    "        input_ids = torch.cat(input_ids_list, dim=0)  # (num_candidates, seq_len)\n",
    "        attention_mask = torch.cat(attn_list, dim=0)  # (num_candidates, seq_len)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        return input_ids.squeeze(1), attention_mask.squeeze(1), labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55351b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 100\n",
      "Number of batches per epoch: 25\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    RerankDataset(rerank_ds, tokenizer),\n",
    "    batch_size=4,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Dataset size: {len(rerank_ds)}\")\n",
    "print(f\"Number of batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aee2e5",
   "metadata": {},
   "source": [
    "## 3. Training loop for LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30f99238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing reranker on device: cuda\n",
      "GPU Memory before model load: 0.91 GB\n",
      "GPU Memory after model load: 0.91 GB\n",
      "Reranker is on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nInitializing reranker on device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU Memory before model load: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "reranker = RerankerModel(model, tokenizer).to(device)\n",
    "\n",
    "# FREEZE BASE PARAMETER\n",
    "for param in reranker.lm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.AdamW(reranker.parameters(), lr=1e-4)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.config.return_dict = True\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "if device == 'cuda':\n",
    "\n",
    "    print(f\"GPU Memory after model load: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"Reranker is on: {next(reranker.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c6ee86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting epoch 1/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 1/100 | Loss: 1.3861 | Batches: 25 | Epoch time: 3.57s | ETA: 0:05:53\n",
      "\n",
      "Starting epoch 2/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 2/100 | Loss: 1.3838 | Batches: 25 | Epoch time: 3.47s | ETA: 0:05:46\n",
      "\n",
      "Starting epoch 3/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 3/100 | Loss: 1.3858 | Batches: 25 | Epoch time: 3.47s | ETA: 0:05:41\n",
      "\n",
      "Starting epoch 4/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 4/100 | Loss: 1.3935 | Batches: 25 | Epoch time: 3.37s | ETA: 0:05:34\n",
      "\n",
      "Starting epoch 5/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 5/100 | Loss: 1.3848 | Batches: 25 | Epoch time: 3.41s | ETA: 0:05:30\n",
      "\n",
      "Starting epoch 6/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 6/100 | Loss: 1.4130 | Batches: 25 | Epoch time: 3.42s | ETA: 0:05:26\n",
      "\n",
      "Starting epoch 7/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 7/100 | Loss: 1.4083 | Batches: 25 | Epoch time: 3.39s | ETA: 0:05:22\n",
      "\n",
      "Starting epoch 8/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 8/100 | Loss: 1.3434 | Batches: 25 | Epoch time: 3.33s | ETA: 0:05:17\n",
      "\n",
      "Starting epoch 9/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 9/100 | Loss: 1.4013 | Batches: 25 | Epoch time: 3.33s | ETA: 0:05:13\n",
      "\n",
      "Starting epoch 10/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 10/100 | Loss: 1.4007 | Batches: 25 | Epoch time: 3.34s | ETA: 0:05:08\n",
      "\n",
      "Starting epoch 11/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 11/100 | Loss: 1.3843 | Batches: 25 | Epoch time: 3.34s | ETA: 0:05:04\n",
      "\n",
      "Starting epoch 12/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 12/100 | Loss: 1.3890 | Batches: 25 | Epoch time: 3.33s | ETA: 0:05:01\n",
      "\n",
      "Starting epoch 13/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 13/100 | Loss: 1.3858 | Batches: 25 | Epoch time: 3.29s | ETA: 0:04:56\n",
      "\n",
      "Starting epoch 14/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 14/100 | Loss: 1.4095 | Batches: 25 | Epoch time: 3.25s | ETA: 0:04:52\n",
      "\n",
      "Starting epoch 15/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 15/100 | Loss: 1.3785 | Batches: 25 | Epoch time: 3.22s | ETA: 0:04:48\n",
      "\n",
      "Starting epoch 16/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 16/100 | Loss: 1.3923 | Batches: 25 | Epoch time: 3.18s | ETA: 0:04:43\n",
      "\n",
      "Starting epoch 17/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 17/100 | Loss: 1.3741 | Batches: 25 | Epoch time: 6.64s | ETA: 0:04:56\n",
      "\n",
      "Starting epoch 18/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 18/100 | Loss: 1.3819 | Batches: 25 | Epoch time: 7.66s | ETA: 0:05:11\n",
      "\n",
      "Starting epoch 19/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 19/100 | Loss: 1.3945 | Batches: 25 | Epoch time: 4.73s | ETA: 0:05:12\n",
      "\n",
      "Starting epoch 20/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 20/100 | Loss: 1.3721 | Batches: 25 | Epoch time: 3.18s | ETA: 0:05:05\n",
      "\n",
      "Starting epoch 21/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 21/100 | Loss: 1.4141 | Batches: 25 | Epoch time: 3.18s | ETA: 0:04:59\n",
      "\n",
      "Starting epoch 22/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 22/100 | Loss: 1.3862 | Batches: 25 | Epoch time: 3.20s | ETA: 0:04:53\n",
      "\n",
      "Starting epoch 23/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 23/100 | Loss: 1.3864 | Batches: 25 | Epoch time: 3.23s | ETA: 0:04:48\n",
      "\n",
      "Starting epoch 24/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 24/100 | Loss: 1.3856 | Batches: 25 | Epoch time: 3.23s | ETA: 0:04:43\n",
      "\n",
      "Starting epoch 25/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 25/100 | Loss: 1.4028 | Batches: 25 | Epoch time: 3.23s | ETA: 0:04:38\n",
      "\n",
      "Starting epoch 26/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 26/100 | Loss: 1.3820 | Batches: 25 | Epoch time: 3.21s | ETA: 0:04:32\n",
      "\n",
      "Starting epoch 27/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 27/100 | Loss: 1.3754 | Batches: 25 | Epoch time: 3.29s | ETA: 0:04:28\n",
      "\n",
      "Starting epoch 28/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 28/100 | Loss: 1.3518 | Batches: 25 | Epoch time: 3.30s | ETA: 0:04:23\n",
      "\n",
      "Starting epoch 29/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 29/100 | Loss: 1.3875 | Batches: 25 | Epoch time: 3.30s | ETA: 0:04:19\n",
      "\n",
      "Starting epoch 30/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 30/100 | Loss: 1.3886 | Batches: 25 | Epoch time: 3.30s | ETA: 0:04:14\n",
      "\n",
      "Starting epoch 31/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 31/100 | Loss: 1.3792 | Batches: 25 | Epoch time: 3.30s | ETA: 0:04:10\n",
      "\n",
      "Starting epoch 32/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 32/100 | Loss: 1.3953 | Batches: 25 | Epoch time: 3.26s | ETA: 0:04:06\n",
      "\n",
      "Starting epoch 33/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 33/100 | Loss: 1.3837 | Batches: 25 | Epoch time: 3.27s | ETA: 0:04:01\n",
      "\n",
      "Starting epoch 34/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 34/100 | Loss: 1.3871 | Batches: 25 | Epoch time: 3.29s | ETA: 0:03:57\n",
      "\n",
      "Starting epoch 35/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 35/100 | Loss: 1.3853 | Batches: 25 | Epoch time: 3.30s | ETA: 0:03:53\n",
      "\n",
      "Starting epoch 36/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 36/100 | Loss: 1.3893 | Batches: 25 | Epoch time: 3.29s | ETA: 0:03:49\n",
      "\n",
      "Starting epoch 37/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 37/100 | Loss: 1.3593 | Batches: 25 | Epoch time: 3.31s | ETA: 0:03:45\n",
      "\n",
      "Starting epoch 38/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 38/100 | Loss: 1.3719 | Batches: 25 | Epoch time: 3.30s | ETA: 0:03:41\n",
      "\n",
      "Starting epoch 39/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 39/100 | Loss: 1.3779 | Batches: 25 | Epoch time: 3.30s | ETA: 0:03:37\n",
      "\n",
      "Starting epoch 40/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 40/100 | Loss: 1.3856 | Batches: 25 | Epoch time: 3.31s | ETA: 0:03:33\n",
      "\n",
      "Starting epoch 41/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 41/100 | Loss: 1.3692 | Batches: 25 | Epoch time: 3.30s | ETA: 0:03:29\n",
      "\n",
      "Starting epoch 42/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 42/100 | Loss: 1.3939 | Batches: 25 | Epoch time: 3.30s | ETA: 0:03:25\n",
      "\n",
      "Starting epoch 43/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 43/100 | Loss: 1.3882 | Batches: 25 | Epoch time: 3.24s | ETA: 0:03:21\n",
      "\n",
      "Starting epoch 44/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 44/100 | Loss: 1.4219 | Batches: 25 | Epoch time: 3.24s | ETA: 0:03:17\n",
      "\n",
      "Starting epoch 45/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 45/100 | Loss: 1.3832 | Batches: 25 | Epoch time: 3.24s | ETA: 0:03:14\n",
      "\n",
      "Starting epoch 46/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 46/100 | Loss: 1.3883 | Batches: 25 | Epoch time: 3.23s | ETA: 0:03:10\n",
      "\n",
      "Starting epoch 47/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 47/100 | Loss: 1.3713 | Batches: 25 | Epoch time: 3.24s | ETA: 0:03:06\n",
      "\n",
      "Starting epoch 48/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 48/100 | Loss: 1.3941 | Batches: 25 | Epoch time: 3.30s | ETA: 0:03:02\n",
      "\n",
      "Starting epoch 49/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 49/100 | Loss: 1.3755 | Batches: 25 | Epoch time: 3.32s | ETA: 0:02:59\n",
      "\n",
      "Starting epoch 50/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 50/100 | Loss: 1.4067 | Batches: 25 | Epoch time: 3.32s | ETA: 0:02:55\n",
      "\n",
      "Starting epoch 51/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 51/100 | Loss: 1.3878 | Batches: 25 | Epoch time: 3.31s | ETA: 0:02:51\n",
      "\n",
      "Starting epoch 52/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 52/100 | Loss: 1.3909 | Batches: 25 | Epoch time: 3.22s | ETA: 0:02:47\n",
      "\n",
      "Starting epoch 53/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 53/100 | Loss: 1.3875 | Batches: 25 | Epoch time: 3.22s | ETA: 0:02:44\n",
      "\n",
      "Starting epoch 54/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 54/100 | Loss: 1.3910 | Batches: 25 | Epoch time: 3.24s | ETA: 0:02:40\n",
      "\n",
      "Starting epoch 55/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 55/100 | Loss: 1.4088 | Batches: 25 | Epoch time: 3.24s | ETA: 0:02:36\n",
      "\n",
      "Starting epoch 56/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 56/100 | Loss: 1.3284 | Batches: 25 | Epoch time: 3.24s | ETA: 0:02:33\n",
      "\n",
      "Starting epoch 57/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 57/100 | Loss: 1.3975 | Batches: 25 | Epoch time: 3.24s | ETA: 0:02:29\n",
      "\n",
      "Starting epoch 58/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 58/100 | Loss: 1.4001 | Batches: 25 | Epoch time: 3.49s | ETA: 0:02:26\n",
      "\n",
      "Starting epoch 59/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 59/100 | Loss: 1.3993 | Batches: 25 | Epoch time: 3.42s | ETA: 0:02:22\n",
      "\n",
      "Starting epoch 60/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 60/100 | Loss: 1.3881 | Batches: 25 | Epoch time: 3.36s | ETA: 0:02:19\n",
      "\n",
      "Starting epoch 61/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 61/100 | Loss: 1.4101 | Batches: 25 | Epoch time: 3.43s | ETA: 0:02:15\n",
      "\n",
      "Starting epoch 62/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 62/100 | Loss: 1.3815 | Batches: 25 | Epoch time: 3.38s | ETA: 0:02:12\n",
      "\n",
      "Starting epoch 63/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 63/100 | Loss: 1.3928 | Batches: 25 | Epoch time: 3.33s | ETA: 0:02:08\n",
      "\n",
      "Starting epoch 64/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 64/100 | Loss: 1.3736 | Batches: 25 | Epoch time: 3.36s | ETA: 0:02:04\n",
      "\n",
      "Starting epoch 65/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 65/100 | Loss: 1.3863 | Batches: 25 | Epoch time: 3.38s | ETA: 0:02:01\n",
      "\n",
      "Starting epoch 66/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 66/100 | Loss: 1.3853 | Batches: 25 | Epoch time: 3.40s | ETA: 0:01:57\n",
      "\n",
      "Starting epoch 67/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 67/100 | Loss: 1.3884 | Batches: 25 | Epoch time: 3.41s | ETA: 0:01:54\n",
      "\n",
      "Starting epoch 68/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 68/100 | Loss: 1.3852 | Batches: 25 | Epoch time: 3.36s | ETA: 0:01:50\n",
      "\n",
      "Starting epoch 69/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 69/100 | Loss: 1.3858 | Batches: 25 | Epoch time: 3.33s | ETA: 0:01:47\n",
      "\n",
      "Starting epoch 70/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 70/100 | Loss: 1.3754 | Batches: 25 | Epoch time: 3.39s | ETA: 0:01:43\n",
      "\n",
      "Starting epoch 71/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 71/100 | Loss: 1.3826 | Batches: 25 | Epoch time: 3.35s | ETA: 0:01:40\n",
      "\n",
      "Starting epoch 72/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 72/100 | Loss: 1.3834 | Batches: 25 | Epoch time: 3.37s | ETA: 0:01:36\n",
      "\n",
      "Starting epoch 73/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 73/100 | Loss: 1.3958 | Batches: 25 | Epoch time: 3.38s | ETA: 0:01:33\n",
      "\n",
      "Starting epoch 74/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 74/100 | Loss: 1.3824 | Batches: 25 | Epoch time: 3.39s | ETA: 0:01:30\n",
      "\n",
      "Starting epoch 75/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 75/100 | Loss: 1.3864 | Batches: 25 | Epoch time: 3.34s | ETA: 0:01:26\n",
      "\n",
      "Starting epoch 76/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 76/100 | Loss: 1.3885 | Batches: 25 | Epoch time: 3.39s | ETA: 0:01:23\n",
      "\n",
      "Starting epoch 77/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 77/100 | Loss: 1.3792 | Batches: 25 | Epoch time: 3.40s | ETA: 0:01:19\n",
      "\n",
      "Starting epoch 78/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 78/100 | Loss: 1.3854 | Batches: 25 | Epoch time: 3.37s | ETA: 0:01:16\n",
      "\n",
      "Starting epoch 79/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 79/100 | Loss: 1.4097 | Batches: 25 | Epoch time: 3.36s | ETA: 0:01:12\n",
      "\n",
      "Starting epoch 80/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 80/100 | Loss: 1.3871 | Batches: 25 | Epoch time: 3.39s | ETA: 0:01:09\n",
      "\n",
      "Starting epoch 81/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 81/100 | Loss: 1.3857 | Batches: 25 | Epoch time: 3.43s | ETA: 0:01:05\n",
      "\n",
      "Starting epoch 82/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 82/100 | Loss: 1.3878 | Batches: 25 | Epoch time: 3.53s | ETA: 0:01:02\n",
      "\n",
      "Starting epoch 83/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 83/100 | Loss: 1.3790 | Batches: 25 | Epoch time: 3.49s | ETA: 0:00:58\n",
      "\n",
      "Starting epoch 84/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 84/100 | Loss: 1.3908 | Batches: 25 | Epoch time: 3.33s | ETA: 0:00:55\n",
      "\n",
      "Starting epoch 85/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 85/100 | Loss: 1.3929 | Batches: 25 | Epoch time: 3.32s | ETA: 0:00:51\n",
      "\n",
      "Starting epoch 86/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 86/100 | Loss: 1.3686 | Batches: 25 | Epoch time: 3.31s | ETA: 0:00:48\n",
      "\n",
      "Starting epoch 87/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 87/100 | Loss: 1.3680 | Batches: 25 | Epoch time: 3.30s | ETA: 0:00:44\n",
      "\n",
      "Starting epoch 88/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 88/100 | Loss: 1.4123 | Batches: 25 | Epoch time: 3.33s | ETA: 0:00:41\n",
      "\n",
      "Starting epoch 89/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 89/100 | Loss: 1.3650 | Batches: 25 | Epoch time: 3.62s | ETA: 0:00:38\n",
      "\n",
      "Starting epoch 90/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 90/100 | Loss: 1.3632 | Batches: 25 | Epoch time: 3.31s | ETA: 0:00:34\n",
      "\n",
      "Starting epoch 91/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 91/100 | Loss: 1.3933 | Batches: 25 | Epoch time: 3.32s | ETA: 0:00:31\n",
      "\n",
      "Starting epoch 92/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 92/100 | Loss: 1.3633 | Batches: 25 | Epoch time: 3.32s | ETA: 0:00:27\n",
      "\n",
      "Starting epoch 93/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 93/100 | Loss: 1.3931 | Batches: 25 | Epoch time: 3.27s | ETA: 0:00:24\n",
      "\n",
      "Starting epoch 94/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 94/100 | Loss: 1.3815 | Batches: 25 | Epoch time: 3.34s | ETA: 0:00:20\n",
      "\n",
      "Starting epoch 95/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 95/100 | Loss: 1.3852 | Batches: 25 | Epoch time: 3.32s | ETA: 0:00:17\n",
      "\n",
      "Starting epoch 96/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 96/100 | Loss: 1.3954 | Batches: 25 | Epoch time: 3.30s | ETA: 0:00:13\n",
      "\n",
      "Starting epoch 97/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 97/100 | Loss: 1.3789 | Batches: 25 | Epoch time: 3.33s | ETA: 0:00:10\n",
      "\n",
      "Starting epoch 98/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 98/100 | Loss: 1.3913 | Batches: 25 | Epoch time: 3.42s | ETA: 0:00:06\n",
      "\n",
      "Starting epoch 99/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 99/100 | Loss: 1.3845 | Batches: 25 | Epoch time: 3.32s | ETA: 0:00:03\n",
      "\n",
      "Starting epoch 100/100\n",
      "GPU Memory at epoch start: 0.91 GB\n",
      "Processing first batch on GPU...\n",
      "Epoch 100/100 | Loss: 1.3920 | Batches: 25 | Epoch time: 3.31s | ETA: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.perf_counter()\n",
    "    print(f\"\\nStarting epoch {epoch + 1}/{num_epochs}\")\n",
    "    if device == 'cuda':\n",
    "        print(f\"GPU Memory at epoch start: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "\n",
    "    reranker.train()\n",
    "    \n",
    "    batch_count = 0\n",
    "    for input_ids, attn_mask, labels in train_loader:\n",
    "        batch_count += 1\n",
    "        if batch_count == 1 and device == 'cuda':\n",
    "            print(f\"Processing first batch on GPU...\")\n",
    "\n",
    "        B, N, T = input_ids.shape\n",
    "        input_ids = input_ids.to(device)\n",
    "        attn_mask = attn_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        flat_ids = input_ids.view(B*N, T)\n",
    "        flat_att = attn_mask.view(B*N, T)\n",
    "\n",
    "        scores = reranker(flat_ids, flat_att)\n",
    "        scores = scores.view(B, N)\n",
    "\n",
    "        loss = criterion(scores, labels.argmax(dim=1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Timing calculations\n",
    "    epoch_time = time.perf_counter() - epoch_start\n",
    "    elapsed = time.perf_counter() - start_time\n",
    "\n",
    "    avg_epoch_time = elapsed / (epoch + 1)\n",
    "    remaining_epochs = num_epochs - (epoch + 1)\n",
    "    eta_seconds = remaining_epochs * avg_epoch_time\n",
    "    eta = timedelta(seconds=int(eta_seconds))\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | Loss: {loss.item():.4f} | \"\n",
    "\n",
    "          f\"Batches: {batch_count} | Epoch time: {epoch_time:.2f}s | \"\n",
    "\n",
    "          f\"ETA: {eta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9fb0f45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(model, context, candidates):\n",
    "    inputs = []\n",
    "    for c in candidates:\n",
    "        txt = context + tokenizer.eos_token + \" \" + c\n",
    "        enc = tokenizer(txt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        inputs.append(enc)\n",
    "\n",
    "    input_ids = torch.cat([x[\"input_ids\"] for x in inputs])\n",
    "    attn = torch.cat([x[\"attention_mask\"] for x in inputs])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores = model(input_ids, attn)\n",
    "\n",
    "    idx = torch.argmax(scores).item()\n",
    "    return candidates[idx], scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
